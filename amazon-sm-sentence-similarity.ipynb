{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorize Accidents Using Sentence Transformer and Linear learner\n",
    "\n",
    "![Workflow](./inference-pipeline.drawio.png)\n",
    "\n",
    "In this notebook, we will demonstrate categorizing accidents using sentence transformer and a simple classifier. \n",
    "\n",
    "First, we fine tune pretrained `bert-base-uncased` model from `HuggingFace Library` in an unsupervised fashion, on `Industrial labor accident data`. The objective is to find the similar accident reports based on the description of the incident using `bert-base-uncased`. \n",
    "\n",
    "Second, we train an linear learner classification model using incident features and similar incidents' features.\n",
    "\n",
    "At the end, we deploy an inference pipeline which takes an incident report as input and predict the accident type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Update sagemaker package and restart the kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker -q\n",
    "# !pip install sentence_transformers -q\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.217.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, os, sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() \n",
    "prefix = 'sentencetransformer/input'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Download the dataset from: https://www.kaggle.com/ihmstefanini/industrial-safety-and-health-analytics-database and upload the downloaded csv file to the notebook. \n",
    "\n",
    "The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is an occurrence of an accident.\n",
    "\n",
    "**Columns description**\n",
    "- Data: timestamp or time/date information\n",
    "- Countries: which country the accident occurred (anonymized)\n",
    "- Local: the city where the manufacturing plant is located (anonymized)\n",
    "- Industry sector: which sector the plant belongs to\n",
    "- Accident level: from I to VI, it registers how severe was the accident (I means not severe but VI means very severe)\n",
    "- Potential Accident Level: Depending on the Accident Level, the database also registers how severe the accident could have been (due to other factors involved in the accident)\n",
    "- Genre: if the person is male of female\n",
    "- Employee or Third Party: if the injured person is an employee or a third party\n",
    "- Critical Risk: some description of the risk involved in the accident\n",
    "- Description: Detailed description of how the accident happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_data = pd.read_csv('IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data['Industry Sector'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data['Genre'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-827930657850/sentencetransformer/input/train.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train.csv')).upload_file('IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv')\n",
    "training_input_path = \"s3://{}/{}/train.csv\".format(bucket,prefix)\n",
    "training_input_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Sentence Transformer on your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 8,\n",
    "                 'model_name':'bert-base-uncased'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "- **SM_MODEL_DIR**: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting. SM_MODEL_DIR is always set to /opt/ml/model.\n",
    "\n",
    "- **SM_NUM_GPUS**: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "- **SM_CHANNEL_XXXX**: A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâ€™s fit call, named train and test, the environment variables SM_CHANNEL_TRAIN and SM_CHANNEL_TEST are set.\n",
    "\n",
    "You can find a full list of the exposed environment variables [here](#https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md).\n",
    "\n",
    "Later we define hyperparameters in the HuggingFace Estimator, which are passed in as named arguments and and can be processed with the [ArgumentParser()](#https://huggingface.co/docs/sagemaker/train#create-an-huggingface-estimator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./code/unsupervised.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='unsupervised.py',\n",
    "                            source_dir='./code',\n",
    "                            instance_type='ml.p3.2xlarge', # GPU supported by Hugging Face\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.6',\n",
    "                            pytorch_version='1.7',\n",
    "                            py_version='py36',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-05-14-15-24-39-469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-14 15:24:40 Starting - Starting the training job...\n",
      "2024-05-14 15:25:07 Pending - Training job waiting for capacity...\n",
      "2024-05-14 15:25:31 Pending - Preparing the instances for training...\n",
      "2024-05-14 15:26:06 Downloading - Downloading input data...\n",
      "2024-05-14 15:26:17 Downloading - Downloading the training image...............\n",
      "2024-05-14 15:28:58 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:26,776 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:26,805 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:26,808 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:26,991 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\u001b[0m\n",
      "\u001b[34mCollecting ipywidgets\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (1.17.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.1.91)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (4.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (7.16.1)\u001b[0m\n",
      "\u001b[34mCollecting widgetsnbextension~=3.6.6\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-widgets<3,>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34mCollecting comm>=0.1.3\n",
      "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (3.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (2.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting ipykernel\n",
      "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-client\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbformat\n",
      "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34mCollecting terminado>=0.8.3\n",
      "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (3.0.1)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-core>=4.6.1\n",
      "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbconvert>=5\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34mCollecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (22.0.3)\u001b[0m\n",
      "\u001b[34mCollecting nest-asyncio>=1.5\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client>=5.3.4->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.9-py3-none-any.whl (69 kB)\u001b[0m\n",
      "\u001b[34mCollecting bleach\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\u001b[0m\n",
      "\u001b[34mCollecting testpath\n",
      "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.21.0,>=1.20.79 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (1.20.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.79->boto3->-r requirements.txt (line 4)) (1.25.11)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (1.14.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.20)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->sentence-transformers->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125919 sha256=5ba82473d514ab1a02c1888c49a78e35bbca2ddbde6d335cee967391c584745a\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/90/11/0e58d454669bc8daf94e04a8da9956aa6f78eb10cddb16dd4e\u001b[0m\n",
      "\u001b[34mSuccessfully built sentence-transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyrsistent, nest-asyncio, jupyter-core, jsonschema, entrypoints, webencodings, nbformat, jupyter-client, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, bleach, argon2-cffi-bindings, terminado, Send2Trash, regex, prometheus-client, nbconvert, ipykernel, argon2-cffi, tokenizers, notebook, huggingface-hub, widgetsnbextension, transformers, nltk, jupyterlab-widgets, comm, sentence-transformers, ipywidgets\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\n",
      "    Uninstalling transformers-4.6.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled transformers-4.6.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed Send2Trash-1.8.3 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 comm-0.1.4 defusedxml-0.7.1 entrypoints-0.4 huggingface-hub-0.4.0 ipykernel-5.5.6 ipywidgets-7.8.1 jsonschema-3.2.0 jupyter-client-7.1.2 jupyter-core-4.9.2 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.7 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.6.0 nltk-3.6.7 notebook-6.4.10 pandocfilters-1.5.1 prometheus-client-0.17.1 pyrsistent-0.18.0 regex-2023.8.8 sentence-transformers-2.2.2 terminado-0.12.1 testpath-0.6.0 tokenizers-0.12.1 transformers-4.18.0 webencodings-0.5.1 widgetsnbextension-3.6.6\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mdatasets 1.6.2 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:44,244 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"bert-base-uncased\",\n",
      "        \"train_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-05-14-15-24-39-469\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-14-15-24-39-469/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"unsupervised\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"unsupervised.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=unsupervised.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=unsupervised\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-14-15-24-39-469/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2024-05-14-15-24-39-469\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-14-15-24-39-469/source/sourcedir.tar.gz\",\"module_name\":\"unsupervised\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"unsupervised.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"bert-base-uncased\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 unsupervised.py --epochs 1 --model_name bert-base-uncased --train_batch_size 8\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:47,101 - filelock - INFO - Lock 140523123699104 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:47,136 - filelock - INFO - Lock 140523123699104 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:47,220 - filelock - INFO - Lock 140523071081048 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:55,188 - filelock - INFO - Lock 140523071081048 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,133 - filelock - INFO - Lock 140523072074696 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,170 - filelock - INFO - Lock 140523072074696 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,262 - filelock - INFO - Lock 140522523479008 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,330 - filelock - INFO - Lock 140522523479008 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,361 - filelock - INFO - Lock 140522523478616 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,422 - filelock - INFO - Lock 140522523478616 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,956 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cuda\u001b[0m\n",
      "\u001b[34m******* list files *********:  ['train.csv']\u001b[0m\n",
      "\u001b[34m********************** Reading Data *************************\n",
      "                  Data  ...                                        Description\u001b[0m\n",
      "\u001b[34m0  2016-01-01 00:00:00  ...  While removing the drill rod of the Jumbo 08 f...\u001b[0m\n",
      "\u001b[34m1  2016-01-02 00:00:00  ...  During the activation of a sodium sulphide pum...\u001b[0m\n",
      "\u001b[34m2  2016-01-06 00:00:00  ...  In the sub-station MILPO located at level +170...\u001b[0m\n",
      "\u001b[34m3  2016-01-08 00:00:00  ...  Being 9:45 am. approximately in the Nv. 1880 C...\u001b[0m\n",
      "\u001b[34m4  2016-01-10 00:00:00  ...  Approximately at 11:45 a.m. in circumstances t...\u001b[0m\n",
      "\u001b[34m[5 rows x 10 columns]\u001b[0m\n",
      "\u001b[34m2024-05-14 15:29:57,999 - sentence_transformers.losses.DenoisingAutoEncoderLoss - WARNING - When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.155 algo-1:39 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.198 algo-1:39 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.198 algo-1:39 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.198 algo-1:39 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.199 algo-1:39 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.199 algo-1:39 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.305 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.306 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.307 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.308 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.309 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.310 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.311 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.312 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.313 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.314 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.315 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.316 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.317 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.318 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.319 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.320 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.321 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.322 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.323 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.324 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.325 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.326 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.327 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.328 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.329 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.330 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.331 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.332 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.333 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.334 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.335 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.336 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.337 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.bias count_params:30522\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.338 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.339 algo-1:39 INFO hook.py:593] Total Trainable Params: 138471738\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.339 algo-1:39 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-05-14 15:30:04.341 algo-1:39 INFO hook.py:488] Hook is writing from the hook with pid: 39\u001b[0m\n",
      "\u001b[34m2024-05-14 15:30:20,781 - sentence_transformers.SentenceTransformer - INFO - Save model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 751kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]#015Downloading:   1%|          | 4.54M/420M [00:00<00:09, 47.6MB/s]#015Downloading:   2%|â–         | 9.34M/420M [00:00<00:08, 48.4MB/s]#015Downloading:   3%|â–Ž         | 14.0M/420M [00:00<00:08, 48.6MB/s]#015Downloading:   5%|â–         | 19.2M/420M [00:00<00:08, 50.0MB/s]#015Downloading:   6%|â–Œ         | 24.4M/420M [00:00<00:08, 51.5MB/s]#015Downloading:   7%|â–‹         | 29.6M/420M [00:00<00:07, 52.3MB/s]#015Downloading:   8%|â–Š         | 34.7M/420M [00:00<00:07, 52.5MB/s]#015Downloading:   9%|â–‰         | 39.9M/420M [00:00<00:07, 53.2MB/s]#015Downloading:  11%|â–ˆ         | 45.2M/420M [00:00<00:07, 53.9MB/s]#015Downloading:  12%|â–ˆâ–        | 50.5M/420M [00:01<00:07, 54.5MB/s]#015Downloading:  13%|â–ˆâ–Ž        | 55.9M/420M [00:01<00:06, 54.9MB/s]#015Downloading:  15%|â–ˆâ–        | 61.1M/420M [00:01<00:06, 54.8MB/s]#015Downloading:  16%|â–ˆâ–Œ        | 66.2M/420M [00:01<00:06, 54.6MB/s]#015Downloading:  17%|â–ˆâ–‹        | 71.5M/420M [00:01<00:06, 54.6MB/s]#015Downloading:  18%|â–ˆâ–Š        | 76.7M/420M [00:01<00:06, 54.7MB/s]#015Downloading:  19%|â–ˆâ–‰        | 81.9M/420M [00:01<00:06, 54.2MB/s]#015Downloading:  21%|â–ˆâ–ˆ        | 87.0M/420M [00:01<00:06, 54.1MB/s]#015Downloading:  22%|â–ˆâ–ˆâ–       | 92.3M/420M [00:01<00:06, 54.5MB/s]#015Downloading:  23%|â–ˆâ–ˆâ–Ž       | 97.6M/420M [00:01<00:06, 54.8MB/s]#015Downloading:  25%|â–ˆâ–ˆâ–       | 103M/420M [00:02<00:06, 55.1MB/s] #015Downloading:  26%|â–ˆâ–ˆâ–Œ       | 108M/420M [00:02<00:05, 55.6MB/s]#015Downloading:  27%|â–ˆâ–ˆâ–‹       | 114M/420M [00:02<00:05, 55.8MB/s]#015Downloading:  28%|â–ˆâ–ˆâ–Š       | 119M/420M [00:02<00:05, 55.3MB/s]#015Downloading:  30%|â–ˆâ–ˆâ–‰       | 124M/420M [00:02<00:05, 55.3MB/s]#015Downloading:  31%|â–ˆâ–ˆâ–ˆ       | 130M/420M [00:02<00:05, 55.5MB/s]#015Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 135M/420M [00:02<00:05, 55.7MB/s]#015Downloading:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 140M/420M [00:02<00:05, 56.1MB/s]#015Downloading:  35%|â–ˆâ–ˆâ–ˆâ–      | 146M/420M [00:02<00:05, 56.1MB/s]#015Downloading:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 151M/420M [00:02<00:05, 56.0MB/s]#015Downloading:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 157M/420M [00:03<00:04, 56.0MB/s]#015Downloading:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 162M/420M [00:03<00:04, 56.1MB/s]#015Downloading:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 167M/420M [00:03<00:04, 56.1MB/s]#015Downloading:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 173M/420M [00:03<00:04, 56.2MB/s]#015Downloading:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 178M/420M [00:03<00:04, 56.2MB/s]#015Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 183M/420M [00:03<00:04, 56.3MB/s]#015Downloading:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 189M/420M [00:03<00:04, 56.4MB/s]#015Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 194M/420M [00:03<00:04, 56.4MB/s]#015Downloading:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 200M/420M [00:03<00:04, 56.4MB/s]#015Downloading:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 205M/420M [00:03<00:04, 56.2MB/s]#015Downloading:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 210M/420M [00:04<00:03, 56.2MB/s]#015Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 216M/420M [00:04<00:03, 56.1MB/s]#015Downloading:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 221M/420M [00:04<00:03, 55.1MB/s]#015Downloading:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 226M/420M [00:04<00:03, 53.8MB/s]#015Downloading:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 232M/420M [00:04<00:03, 54.4MB/s]#015Downloading:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 237M/420M [00:04<00:03, 54.9MB/s]#015Downloading:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 242M/420M [00:04<00:03, 55.2MB/s]#015Downloading:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 248M/420M [00:04<00:03, 55.5MB/s]#015Downloading:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 253M/420M [00:04<00:03, 55.6MB/s]#015Downloading:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 258M/420M [00:04<00:03, 55.9MB/s]#015Downloading:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 264M/420M [00:05<00:02, 56.0MB/s]#015Downloading:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 269M/420M [00:05<00:02, 55.9MB/s]#015Downloading:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 274M/420M [00:05<00:02, 55.9MB/s]#015Downloading:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 280M/420M [00:05<00:02, 55.8MB/s]#015Downloading:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 285M/420M [00:05<00:02, 55.8MB/s]#015Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 290M/420M [00:05<00:02, 56.1MB/s]#015Downloading:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 296M/420M [00:05<00:02, 56.2MB/s]#015Downloading:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 301M/420M [00:05<00:02, 56.2MB/s]#015Downloading:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 307M/420M [00:05<00:02, 56.3MB/s]#015Downloading:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 312M/420M [00:05<00:02, 56.3MB/s]#015Downloading:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 317M/420M [00:06<00:01, 56.3MB/s]#015Downloading:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 323M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 328M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 333M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 339M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 344M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 349M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 355M/420M [00:06<00:01, 56.0MB/s]#015Downloading:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 360M/420M [00:06<00:01, 56.1MB/s]#015Downloading:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 366M/420M [00:06<00:01, 56.2MB/s]#015Downloading:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 371M/420M [00:07<00:00, 56.3MB/s]#015Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 376M/420M [00:07<00:00, 56.3MB/s]#015Downloading:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 382M/420M [00:07<00:00, 56.2MB/s]#015Downloading:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 387M/420M [00:07<00:00, 56.3MB/s]#015Downloading:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 392M/420M [00:07<00:00, 56.1MB/s]#015Downloading:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 398M/420M [00:07<00:00, 56.2MB/s]#015Downloading:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 403M/420M [00:07<00:00, 56.3MB/s]#015Downloading:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 409M/420M [00:07<00:00, 56.2MB/s]#015Downloading:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 414M/420M [00:07<00:00, 56.2MB/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 419M/420M [00:07<00:00, 56.3MB/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 420M/420M [00:07<00:00, 55.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 53.1kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226k/226k [00:00<00:00, 37.8MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455k/455k [00:00<00:00, 31.0MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mThe following encoder weights were not tied to the decoder ['bert/pooler']\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/54 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2024-05-14 15:30:22,067 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015Iteration:   2%|â–         | 1/54 [00:01<01:09,  1.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:   4%|â–Ž         | 2/54 [00:01<01:05,  1.27s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:   6%|â–Œ         | 3/54 [00:01<01:02,  1.22s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:   7%|â–‹         | 4/54 [00:02<00:58,  1.17s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:   9%|â–‰         | 5/54 [00:02<00:55,  1.13s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  11%|â–ˆ         | 6/54 [00:02<00:52,  1.08s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  13%|â–ˆâ–Ž        | 7/54 [00:03<00:49,  1.05s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  15%|â–ˆâ–        | 8/54 [00:03<00:46,  1.01s/it]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  17%|â–ˆâ–‹        | 9/54 [00:03<00:43,  1.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  19%|â–ˆâ–Š        | 10/54 [00:04<00:41,  1.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  20%|â–ˆâ–ˆ        | 11/54 [00:04<00:39,  1.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  22%|â–ˆâ–ˆâ–       | 12/54 [00:04<00:36,  1.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  24%|â–ˆâ–ˆâ–       | 13/54 [00:04<00:34,  1.18it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  26%|â–ˆâ–ˆâ–Œ       | 14/54 [00:05<00:32,  1.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  28%|â–ˆâ–ˆâ–Š       | 15/54 [00:05<00:30,  1.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  30%|â–ˆâ–ˆâ–‰       | 16/54 [00:05<00:29,  1.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  31%|â–ˆâ–ˆâ–ˆâ–      | 17/54 [00:06<00:27,  1.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 18/54 [00:06<00:25,  1.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 19/54 [00:06<00:24,  1.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 20/54 [00:06<00:23,  1.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 21/54 [00:07<00:21,  1.52it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 22/54 [00:07<00:20,  1.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 23/54 [00:07<00:19,  1.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 24/54 [00:08<00:18,  1.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 25/54 [00:08<00:17,  1.70it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 26/54 [00:08<00:16,  1.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/54 [00:08<00:15,  1.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 28/54 [00:09<00:14,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 29/54 [00:09<00:13,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 30/54 [00:09<00:12,  1.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 31/54 [00:10<00:11,  1.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 32/54 [00:10<00:10,  2.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 33/54 [00:10<00:10,  2.05it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 34/54 [00:10<00:09,  2.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 35/54 [00:11<00:08,  2.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 36/54 [00:11<00:08,  2.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 37/54 [00:11<00:07,  2.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 38/54 [00:12<00:07,  2.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 39/54 [00:12<00:06,  2.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 40/54 [00:12<00:05,  2.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 41/54 [00:12<00:05,  2.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 42/54 [00:13<00:04,  2.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 43/54 [00:13<00:04,  2.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 44/54 [00:13<00:03,  2.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 45/54 [00:14<00:03,  2.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 46/54 [00:14<00:03,  2.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 47/54 [00:14<00:02,  2.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 48/54 [00:14<00:02,  2.64it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 49/54 [00:15<00:01,  2.66it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 50/54 [00:15<00:01,  2.71it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 51/54 [00:15<00:01,  2.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 52/54 [00:16<00:00,  2.76it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 53/54 [00:16<00:00,  2.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:16<00:00,  2.85it/s]#033[A#015Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:16<00:00,  3.26it/s]\u001b[0m\n",
      "\u001b[34m#015Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.58s/it]#015Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.58s/it]\u001b[0m\n",
      "\n",
      "2024-05-14 15:30:28 Uploading - Uploading generated training model\n",
      "2024-05-14 15:30:46 Completed - Training job completed\n",
      "Training seconds: 285\n",
      "Billable seconds: 285\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-14-15-24-39-469/output/model.tar.gz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Fine-tuned Sentence Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy the `Sentence Transformer` model using `SageMaker HuggingFaceModel` object with `inference.py` script as an entrypoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look into the `inference` script which is in the `code` directory and add the bucket name where you have the training data. Also, don't forget to update the `s3key`. This data will act as the source data, against which we will compare our target sentence. In this case, based on the description of the incident, model will find the similar accident reports.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize ./code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "sentence_transformer = HuggingFaceModel(model_data = huggingface_estimator.model_data, \n",
    "                                    role = role, \n",
    "                                    source_dir = 'code',\n",
    "                                    entry_point = 'inference.py', \n",
    "                                    transformers_version='4.6',\n",
    "                                    pytorch_version='1.7',\n",
    "                                    py_version='py36',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy endpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = sentence_transformer.deploy(initial_instance_count = 1, instance_type = 'ml.g4dn.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = predictor.predict(\"they saw the bee carton, the reaction was to move away from the box as quickly as possible to avoid the stings, they ran about 50 meters, looking for a safe area, to exit the radius of attack of the bees, but the S.S. and Breno), were attacked and consequently they suffered 02 stings, in the belly and Jehovah in the hand, verified that there was no type of allergic reaction, returned with the normal activities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the returned prediction is in json format, can add output_fn function in inference script to covert it to csv format\n",
    "result = json.loads(prediction)\n",
    "result = result['result']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.2xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = sess.boto_session.client(\"sagemaker\")\n",
    "sm_client.delete_endpoint(EndpointName='huggingface-pytorch-inference-2024-04-10-17-48-52-730')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_input_path = \"s3://{}/{}/incident-batch.jsonl\".format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_output_path = \"s3://{}/{}\".format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-827930657850/sentencetransformer/input'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = sentence_transformer.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=testing_output_path,\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "\n",
    "batch_job.transform(\n",
    "    data=testing_input_path,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"1.2-1\"\n",
    "script_path = \"feature-processing.py\"\n",
    "\n",
    "sklearn_preprocessor = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sklearn_preprocessor.fit({\"train\": 's3://sagemaker-us-east-1-827930657850/sentencetransformer/input'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input = 's3://sagemaker-us-east-1-827930657850/sentencetransformer/input/train.csv'\n",
    "transformer = sklearn_preprocessor.transformer(\n",
    "    instance_count=1, instance_type=\"ml.m5.xlarge\", assemble_with=\"Line\", accept=\"text/csv\"\n",
    ")\n",
    "# Preprocess training input\n",
    "transformer.transform(train_input, content_type=\"text/csv\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()\n",
    "preprocessed_train = transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "ll_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "s3_ll_output_key_prefix = \"ll_training_output\"\n",
    "s3_ll_output_location = \"s3://{}/{}/{}/{}\".format(\n",
    "    bucket, prefix, s3_ll_output_key_prefix, \"ll_model\"\n",
    ")\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(\n",
    "    ll_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_ll_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(feature_dim=4, predictor_type=\"regressor\", mini_batch_size=1)\n",
    "\n",
    "ll_train_data = sagemaker.inputs.TrainingInput(\n",
    "    preprocessed_train,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "data_channels = {\"train\": ll_train_data}\n",
    "ll_estimator.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "scikit_learn_inference_model = sklearn_preprocessor.create_model()\n",
    "linear_learner_model = ll_estimator.create_model()\n",
    "\n",
    "model_name = \"inference-pipeline-\" + timestamp_prefix\n",
    "endpoint_name = \"inference-pipeline-ep-\" + timestamp_prefix\n",
    "sm_model = PipelineModel(\n",
    "    name=model_name, role=role, models=[sentence_transformer, scikit_learn_inference_model]#, linear_learner_model]\n",
    ")\n",
    "\n",
    "sm_model.deploy(initial_instance_count=1, instance_type='ml.g4dn.2xlarge', endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "payload = \"they saw the bee carton, the reaction was to move away from the box as quickly as possible to avoid the stings, they ran about 50 meters, looking for a safe area, to exit the radius of attack of the bees, but the S.S. and Breno), were attacked and consequently they suffered 02 stings, in the belly and Jehovah in the hand, verified that there was no type of allergic reaction, returned with the normal activities.\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name, sagemaker_session=sess, serializer=CSVSerializer()\n",
    ")\n",
    "\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Fetched defaults config from location: /root/sagemaker-inference-pipeline-byos\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-pytorch-training-2024-05-15-21-42-24-718\n",
      "2024-05-15 21:42:26 Starting - Starting the training job...\n",
      "2024-05-15 21:42:27 Pending - Training job waiting for capacity..................\n",
      "2024-05-15 21:45:41 Pending - Preparing the instances for training...\n",
      "2024-05-15 21:46:18 Downloading - Downloading input data...\n",
      "2024-05-15 21:46:38 Downloading - Downloading the training image...............\n",
      "2024-05-15 21:49:09 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:37,629 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:37,661 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:37,664 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:37,841 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\u001b[0m\n",
      "\u001b[34mCollecting ipywidgets\n",
      "  Downloading ipywidgets-7.8.1-py2.py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (1.17.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.5.4)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.1.91)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 1)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (2021.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (0.10.3)\u001b[0m\n",
      "\u001b[34mCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers->-r requirements.txt (line 1)) (0.0.45)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (4.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (7.16.1)\u001b[0m\n",
      "\u001b[34mCollecting widgetsnbextension~=3.6.6\n",
      "  Downloading widgetsnbextension-3.6.6-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-widgets<3,>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.7-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34mCollecting comm>=0.1.3\n",
      "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.6/site-packages (from ipywidgets->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (2.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (3.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pexpect in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (49.6.0.post20210108)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->-r requirements.txt (line 2)) (0.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.3.1->ipywidgets->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting notebook>=4.4.1\n",
      "  Downloading notebook-6.4.10-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi\n",
      "  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (3.0.1)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-core>=4.6.1\n",
      "  Downloading jupyter_core-4.9.2-py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (6.1)\u001b[0m\n",
      "\u001b[34mCollecting jupyter-client>=5.3.4\n",
      "  Downloading jupyter_client-7.1.2-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbformat\n",
      "  Downloading nbformat-5.1.3-py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbconvert>=5\n",
      "  Downloading nbconvert-6.0.7-py3-none-any.whl (552 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (22.0.3)\u001b[0m\n",
      "\u001b[34mCollecting ipykernel\n",
      "  Downloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus-client\n",
      "  Downloading prometheus_client-0.17.1-py3-none-any.whl (60 kB)\u001b[0m\n",
      "\u001b[34mCollecting terminado>=0.8.3\n",
      "  Downloading terminado-0.12.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting Send2Trash>=1.8.0\n",
      "  Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting nest-asyncio>=1.5\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter-client>=5.3.4->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting entrypoints\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting testpath\n",
      "  Downloading testpath-0.6.0-py3-none-any.whl (83 kB)\u001b[0m\n",
      "\u001b[34mCollecting mistune<2,>=0.8.1\n",
      "  Downloading mistune-0.8.4-py2.py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting bleach\n",
      "  Downloading bleach-4.1.0-py2.py3-none-any.whl (157 kB)\u001b[0m\n",
      "\u001b[34mCollecting defusedxml\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting nbclient<0.6.0,>=0.5.0\n",
      "  Downloading nbclient-0.5.9-py3-none-any.whl (69 kB)\u001b[0m\n",
      "\u001b[34mCollecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.1.2-py2.py3-none-any.whl (4.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mCollecting async-generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting jsonschema!=2.5.0,>=2.4\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyrsistent>=0.14.0\n",
      "  Downloading pyrsistent-0.18.0-cp36-cp36m-manylinux1_x86_64.whl (117 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ptyprocess in /opt/conda/lib/python3.6/site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 3)) (2021.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.21.0,>=1.20.79 in /opt/conda/lib/python3.6/site-packages (from boto3->-r requirements.txt (line 4)) (1.20.79)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.79->boto3->-r requirements.txt (line 4)) (1.25.11)\u001b[0m\n",
      "\u001b[34mCollecting argon2-cffi-bindings\n",
      "  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (1.14.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets->-r requirements.txt (line 2)) (2.20)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2023.8.8-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->sentence-transformers->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->sentence-transformers->-r requirements.txt (line 1)) (8.2.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125919 sha256=5457eb8281c8a9783674fdf5d262778095052e1eb9eed202ef4a4594ed0fd91c\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/90/11/0e58d454669bc8daf94e04a8da9956aa6f78eb10cddb16dd4e\u001b[0m\n",
      "\u001b[34mSuccessfully built sentence-transformers\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyrsistent, nest-asyncio, jupyter-core, jsonschema, entrypoints, webencodings, nbformat, jupyter-client, async-generator, testpath, pandocfilters, nbclient, mistune, jupyterlab-pygments, defusedxml, bleach, argon2-cffi-bindings, terminado, Send2Trash, regex, prometheus-client, nbconvert, ipykernel, argon2-cffi, tokenizers, notebook, huggingface-hub, widgetsnbextension, transformers, nltk, jupyterlab-widgets, comm, sentence-transformers, ipywidgets\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.3\n",
      "    Uninstalling tokenizers-0.10.3:\n",
      "      Successfully uninstalled tokenizers-0.10.3\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.8\n",
      "    Uninstalling huggingface-hub-0.0.8:\n",
      "      Successfully uninstalled huggingface-hub-0.0.8\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.6.1\n",
      "    Uninstalling transformers-4.6.1:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled transformers-4.6.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed Send2Trash-1.8.3 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 async-generator-1.10 bleach-4.1.0 comm-0.1.4 defusedxml-0.7.1 entrypoints-0.4 huggingface-hub-0.4.0 ipykernel-5.5.6 ipywidgets-7.8.1 jsonschema-3.2.0 jupyter-client-7.1.2 jupyter-core-4.9.2 jupyterlab-pygments-0.1.2 jupyterlab-widgets-1.1.7 mistune-0.8.4 nbclient-0.5.9 nbconvert-6.0.7 nbformat-5.1.3 nest-asyncio-1.6.0 nltk-3.6.7 notebook-6.4.10 pandocfilters-1.5.1 prometheus-client-0.17.1 pyrsistent-0.18.0 regex-2023.8.8 sentence-transformers-2.2.2 terminado-0.12.1 testpath-0.6.0 tokenizers-0.12.1 transformers-4.18.0 webencodings-0.5.1 widgetsnbextension-3.6.6\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mdatasets 1.6.2 requires huggingface-hub<0.1.0, but you have huggingface-hub 0.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:55,429 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"bert-base-uncased\",\n",
      "        \"train_batch_size\": 8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2024-05-15-21-42-24-718\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-15-21-42-24-718/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-15-21-42-24-718/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"bert-base-uncased\",\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2024-05-15-21-42-24-718\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-827930657850/huggingface-pytorch-training-2024-05-15-21-42-24-718/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"bert-base-uncased\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --epochs 1 --model_name bert-base-uncased --train_batch_size 8\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:58,327 - filelock - INFO - Lock 140209797197104 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:58,366 - filelock - INFO - Lock 140209797197104 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:49:58,457 - filelock - INFO - Lock 140209744628200 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:05,716 - filelock - INFO - Lock 140209744628200 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:07,688 - filelock - INFO - Lock 140209745613656 acquired on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:07,764 - filelock - INFO - Lock 140209745613656 released on /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:07,878 - filelock - INFO - Lock 140209197031320 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:07,921 - filelock - INFO - Lock 140209197031320 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:07,987 - filelock - INFO - Lock 140209197029528 acquired on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:08,059 - filelock - INFO - Lock 140209197029528 released on /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:08,633 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device: cuda\u001b[0m\n",
      "\u001b[34m******* list files *********:  ['train.csv']\u001b[0m\n",
      "\u001b[34m********************** Reading Data *************************\n",
      "                  Data  ...                                        Description\u001b[0m\n",
      "\u001b[34m0  2016-01-01 00:00:00  ...  While removing the drill rod of the Jumbo 08 f...\u001b[0m\n",
      "\u001b[34m1  2016-01-02 00:00:00  ...  During the activation of a sodium sulphide pum...\u001b[0m\n",
      "\u001b[34m2  2016-01-06 00:00:00  ...  In the sub-station MILPO located at level +170...\u001b[0m\n",
      "\u001b[34m3  2016-01-08 00:00:00  ...  Being 9:45 am. approximately in the Nv. 1880 C...\u001b[0m\n",
      "\u001b[34m4  2016-01-10 00:00:00  ...  Approximately at 11:45 a.m. in circumstances t...\u001b[0m\n",
      "\u001b[34m[5 rows x 10 columns]\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:08,677 - sentence_transformers.losses.DenoisingAutoEncoderLoss - WARNING - When tie_encoder_decoder=True, the decoder_name_or_path will be invalid.\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.678 algo-1:39 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.721 algo-1:39 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.722 algo-1:39 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.722 algo-1:39 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.723 algo-1:39 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.723 algo-1:39 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.831 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.831 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.831 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.832 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.833 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.834 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.835 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.836 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.837 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.838 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.839 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.840 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.841 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.842 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.843 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.844 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.845 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.846 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.847 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.848 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.849 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.850 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:encoder.0.auto_model.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.851 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.852 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.853 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.854 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.855 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.856 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.857 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.858 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.859 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.860 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.861 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.862 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.863 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.bias count_params:30522\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.864 algo-1:39 INFO hook.py:591] name:decoder.cls.predictions.transform.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.865 algo-1:39 INFO hook.py:593] Total Trainable Params: 138471738\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.865 algo-1:39 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2024-05-15 21:50:14.867 algo-1:39 INFO hook.py:488] Hook is writing from the hook with pid: 39\u001b[0m\n",
      "\u001b[34m2024-05-15 21:50:31,739 - sentence_transformers.SentenceTransformer - INFO - Save model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 638kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]#015Downloading:   1%|â–         | 5.31M/420M [00:00<00:07, 55.7MB/s]#015Downloading:   3%|â–Ž         | 10.7M/420M [00:00<00:07, 56.0MB/s]#015Downloading:   4%|â–         | 16.3M/420M [00:00<00:07, 56.7MB/s]#015Downloading:   5%|â–Œ         | 21.9M/420M [00:00<00:07, 57.3MB/s]#015Downloading:   7%|â–‹         | 27.6M/420M [00:00<00:07, 58.0MB/s]#015Downloading:   8%|â–Š         | 33.3M/420M [00:00<00:06, 58.5MB/s]#015Downloading:   9%|â–‰         | 39.1M/420M [00:00<00:06, 59.2MB/s]#015Downloading:  11%|â–ˆ         | 44.9M/420M [00:00<00:06, 59.7MB/s]#015Downloading:  12%|â–ˆâ–        | 50.8M/420M [00:00<00:06, 60.2MB/s]#015Downloading:  13%|â–ˆâ–Ž        | 56.6M/420M [00:01<00:06, 60.5MB/s]#015Downloading:  15%|â–ˆâ–        | 62.5M/420M [00:01<00:06, 60.8MB/s]#015Downloading:  16%|â–ˆâ–‹        | 68.4M/420M [00:01<00:06, 61.0MB/s]#015Downloading:  18%|â–ˆâ–Š        | 74.2M/420M [00:01<00:05, 61.2MB/s]#015Downloading:  19%|â–ˆâ–‰        | 80.1M/420M [00:01<00:05, 61.4MB/s]#015Downloading:  20%|â–ˆâ–ˆ        | 86.0M/420M [00:01<00:05, 61.5MB/s]#015Downloading:  22%|â–ˆâ–ˆâ–       | 91.9M/420M [00:01<00:05, 61.5MB/s]#015Downloading:  23%|â–ˆâ–ˆâ–Ž       | 97.8M/420M [00:01<00:05, 61.7MB/s]#015Downloading:  25%|â–ˆâ–ˆâ–       | 104M/420M [00:01<00:05, 61.7MB/s] #015Downloading:  26%|â–ˆâ–ˆâ–Œ       | 110M/420M [00:01<00:05, 61.8MB/s]#015Downloading:  27%|â–ˆâ–ˆâ–‹       | 115M/420M [00:02<00:05, 61.8MB/s]#015Downloading:  29%|â–ˆâ–ˆâ–‰       | 121M/420M [00:02<00:05, 61.8MB/s]#015Downloading:  30%|â–ˆâ–ˆâ–ˆ       | 127M/420M [00:02<00:04, 61.6MB/s]#015Downloading:  32%|â–ˆâ–ˆâ–ˆâ–      | 133M/420M [00:02<00:04, 61.5MB/s]#015Downloading:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 139M/420M [00:02<00:04, 61.6MB/s]#015Downloading:  35%|â–ˆâ–ˆâ–ˆâ–      | 145M/420M [00:02<00:04, 61.4MB/s]#015Downloading:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 151M/420M [00:02<00:04, 61.6MB/s]#015Downloading:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 157M/420M [00:02<00:04, 61.6MB/s]#015Downloading:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 163M/420M [00:02<00:04, 61.7MB/s]#015Downloading:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 169M/420M [00:02<00:04, 61.7MB/s]#015Downloading:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 174M/420M [00:03<00:04, 61.8MB/s]#015Downloading:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 180M/420M [00:03<00:04, 61.8MB/s]#015Downloading:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 186M/420M [00:03<00:03, 61.8MB/s]#015Downloading:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 192M/420M [00:03<00:03, 61.8MB/s]#015Downloading:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 198M/420M [00:03<00:03, 60.4MB/s]#015Downloading:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 204M/420M [00:03<00:03, 59.0MB/s]#015Downloading:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 209M/420M [00:03<00:03, 58.4MB/s]#015Downloading:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 215M/420M [00:03<00:03, 58.3MB/s]#015Downloading:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 221M/420M [00:03<00:03, 59.0MB/s]#015Downloading:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 227M/420M [00:03<00:03, 59.6MB/s]#015Downloading:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 232M/420M [00:04<00:03, 60.0MB/s]#015Downloading:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 238M/420M [00:04<00:03, 60.4MB/s]#015Downloading:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 244M/420M [00:04<00:03, 60.6MB/s]#015Downloading:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 250M/420M [00:04<00:02, 60.8MB/s]#015Downloading:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 256M/420M [00:04<00:02, 60.0MB/s]#015Downloading:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 261M/420M [00:04<00:02, 58.3MB/s]#015Downloading:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 267M/420M [00:04<00:02, 57.7MB/s]#015Downloading:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 273M/420M [00:04<00:02, 56.1MB/s]#015Downloading:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 278M/420M [00:04<00:02, 56.1MB/s]#015Downloading:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 284M/420M [00:04<00:02, 57.8MB/s]#015Downloading:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 290M/420M [00:05<00:02, 59.1MB/s]#015Downloading:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 296M/420M [00:05<00:02, 60.0MB/s]#015Downloading:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 302M/420M [00:05<00:02, 59.2MB/s]#015Downloading:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 307M/420M [00:05<00:01, 59.9MB/s]#015Downloading:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 313M/420M [00:05<00:01, 60.7MB/s]#015Downloading:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 319M/420M [00:05<00:01, 61.3MB/s]#015Downloading:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 325M/420M [00:05<00:01, 61.5MB/s]#015Downloading:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 331M/420M [00:05<00:01, 61.9MB/s]#015Downloading:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 337M/420M [00:05<00:01, 62.2MB/s]#015Downloading:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 343M/420M [00:05<00:01, 62.4MB/s]#015Downloading:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 349M/420M [00:06<00:01, 62.6MB/s]#015Downloading:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 355M/420M [00:06<00:01, 62.9MB/s]#015Downloading:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 361M/420M [00:06<00:00, 63.0MB/s]#015Downloading:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 367M/420M [00:06<00:00, 63.0MB/s]#015Downloading:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 373M/420M [00:06<00:00, 63.0MB/s]#015Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 379M/420M [00:06<00:00, 62.9MB/s]#015Downloading:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 385M/420M [00:06<00:00, 62.8MB/s]#015Downloading:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 391M/420M [00:06<00:00, 62.9MB/s]#015Downloading:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 397M/420M [00:06<00:00, 63.0MB/s]#015Downloading:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 404M/420M [00:06<00:00, 63.1MB/s]#015Downloading:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 410M/420M [00:07<00:00, 63.1MB/s]#015Downloading:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 416M/420M [00:07<00:00, 63.2MB/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 420M/420M [00:07<00:00, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00<00:00, 48.8kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226k/226k [00:00<00:00, 37.4MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 455k/455k [00:00<00:00, 14.0MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias',"
     ]
    }
   ],
   "source": [
    "!python pipeline.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## (Optional) Alternative way of processing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='newsela-skprocessing'\n",
    ")\n",
    "\n",
    "# run as a processing job\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_input_path, \n",
    "            destination=\"/opt/ml/processing/input/report\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=similar_report_path, \n",
    "            destination=\"/opt/ml/processing/input/similar_report\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=train_path,\n",
    "        ),\n",
    "        ProcessingOutput(output_name=\"validation_data\", source=\"/opt/ml/processing/output/validation\", destination=validation_path),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/output/test\", destination=test_path),\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
